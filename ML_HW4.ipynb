{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss of least square regression:$$ (y_i - \\hat{y}_i)^2 $$\n",
    "gradient = $$ 2(\\hat{y}_i - y_i) $$\n",
    "loss in logistic: $$ l(y, \\hat{y}) = y \\log(1 + \\exp(-\\hat{y})) + (1 - y) \\log(1 + \\exp(\\hat{y})) $$  \n",
    "gradient of log_loss: $$ g = 1/(1 + \\exp(-\\hat{y})) - y $$\n",
    "hessian of log_loss: $$ h = 1/(1 + \\exp(-\\hat{y})) * (1- 1/(1 + \\exp(-\\hat{y})))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return 2*(score - true)\n",
    "\n",
    "    def h(self,true,score):\n",
    "#         return 2\n",
    "        return np.ones_like(score) * 2\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,score):\n",
    "        return 1 / (1 + np.exp(-score))\n",
    "        \n",
    "\n",
    "    def g(self,true,score):\n",
    "        pred = self.pred(score)\n",
    "        return (pred - true)\n",
    "\n",
    "    def h(self,true,score):\n",
    "        pred = self.pred(score)\n",
    "        return pred * (1 - pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    \n",
    "    #TODO\n",
    "    def __init__(self, depth = None, split_feature = None, split_threshold = None, left_child = None, right_child = None, prediction_value = None):\n",
    "        \n",
    "#         # store essential information in every tree node\n",
    "#         self.X = X #features associated with node\n",
    "#         self.y = y\n",
    "        self.depth = depth\n",
    "        self.is_leaf = left_child is None and right_child is None\n",
    "        self.split_feature = split_feature\n",
    "        self.split_threshold = split_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.prediction_value = prediction_value\n",
    "#         self.prediction_probs = None\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads = None, \n",
    "                 max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 1, gamma = 0, rf = 0):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = 0\n",
    "        self.int_member = 0\n",
    "\n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        #TODO\n",
    "#         if g is None:\n",
    "#             g = np.zeros_like(train[:, -1])\n",
    "#         if h is None:\n",
    "#             h = np.zeros_like(train[:, -1])\n",
    "            \n",
    "        self.tree = self.construct_tree(train, g, h, self.max_depth)\n",
    "#         self.int_member = np.mean(train[:, -1])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        result = []\n",
    "        num_samples = test.shape[0]\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            instance = test[i]\n",
    "            node = self.tree\n",
    "            while node.is_leaf is False:\n",
    "                node = node.left_child if instance[node.split_feature] < node.split_threshold else node.right_child\n",
    "            result.append(node.prediction_value)\n",
    "        return np.array(result)\n",
    "    \n",
    "    \n",
    "    def construct_tree(self, train, g, h, max_depth):\n",
    "#         prediction_value = np.mean(train[:, -1])  # Calculate the mean of target values y\n",
    "        \n",
    "        num_samples = train.shape[0]\n",
    "#         total_gradient, total_hessian = np.sum(g), np.sum(h)\n",
    "        sum_g = np.sum(g)\n",
    "        sum_h = np.sum(h)\n",
    "        prediction_value = -sum_g / (sum_h + self.lamda)\n",
    "\n",
    "        \n",
    "        #TODO      \n",
    "        if train.shape[0] < self.min_sample_split or max_depth <= 0:\n",
    "            return TreeNode(prediction_value=prediction_value)\n",
    "\n",
    "        \n",
    "        feature, threshold, gain = self.find_best_decision_rule(train, g, h)\n",
    "        \n",
    "        if gain <= 0:\n",
    "            return TreeNode(prediction_value=prediction_value)\n",
    "        \n",
    "        if threshold is None:\n",
    "            raise ValueError(\"Threshold cannot be None\")\n",
    "            \n",
    "        #split node\n",
    "        left_indices = train[:, feature] < threshold\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        #recursively apply construct_tree function until above conditions triggered\n",
    "        left_child = self.construct_tree(train[left_indices], g[left_indices], h[left_indices], max_depth - 1)\n",
    "        right_child = self.construct_tree(train[right_indices], g[right_indices], h[right_indices], max_depth - 1)\n",
    "\n",
    "        return TreeNode(split_feature = feature, split_threshold = threshold, \n",
    "                        left_child = left_child, right_child = right_child)\n",
    "\n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        #TODO 写了\n",
    "        best_gain = -float('inf')\n",
    "        best_feature = 0\n",
    "        best_threshold = 0\n",
    "        \n",
    "        for feature in range(train.shape[1]): #for X\n",
    "            threshold, gain = self.find_threshold(g, h, train[:, feature])\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                \n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "    def find_threshold(self, g, h, train):\n",
    "        '''\n",
    "        Given a particular feature $p_j$,\n",
    "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        #TODO\n",
    "        num = train.shape[0] #number of samples\n",
    "        sorted_indices = np.argsort(train)\n",
    "        sorted_values = train[sorted_indices]\n",
    "        best_threshold = 0\n",
    "        best_gain = -float('inf') # initialize gain with small number, update and maximize\n",
    "        sorted_g = g[sorted_indices]\n",
    "        sorted_h = h[sorted_indices]\n",
    "        \n",
    "        sum_g = np.sum(sorted_g)\n",
    "        sum_h = np.sum(sorted_h)\n",
    "        l_g, l_h, r_g, r_h = 0, 0, sum_g, sum_h\n",
    "        \n",
    "        for i in range(1, num):\n",
    "            \n",
    "            l_g += sorted_g[i - 1]\n",
    "            r_g -= sorted_g[i - 1]\n",
    "            l_h += sorted_h[i - 1]\n",
    "            r_h -= sorted_h[i - 1]\n",
    "\n",
    "            if sorted_values[i] != sorted_values[i - 1]:\n",
    "                gain = 0.5 * ((l_g ** 2 / (l_h + self.lamda)) + (r_g ** 2 / (r_h + self.lamda)) -(sum_g ** 2 / (sum_h + self.lamda))) - self.gamma\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_threshold = (sorted_values[i] + sorted_values[i - 1]) / 2\n",
    "                    best_gain = gain\n",
    "\n",
    "        \n",
    "        return [best_threshold, best_gain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class of Random Forest\n",
    "    \n",
    "Parameters:\n",
    "    n_threads: The number of threads used for fitting and predicting.  \n",
    "    loss: Loss function for gradient boosting.  \n",
    "        'mse' for regression task and 'log' for classfication task.  \n",
    "        A child class of the loss class could be passed to implement customized loss.  \n",
    "    max_depth: The maximum depth d_max of a tree.  \n",
    "    min_sample_split: The minimum number of samples required to further split a node.  \n",
    "    lamda: The regularization coefficient for leaf score, also known as lambda.  \n",
    "    gamma: The regularization coefficient for number of tree nodes, also know as gamma.  \n",
    "    rf: rf*m is the size of random subset of features, from which we select the best decision rule.  \n",
    "    num_trees: Number of trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        rf = 0.99, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss            \n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "#         self.avg_prediction = None\n",
    "\n",
    "    def fit(self, train, target): # train is n x m 2d numpy array; target is n-dim 1d array\n",
    "        \n",
    "        #TODO\n",
    "        #score is prediction\n",
    "#         score = np.zeros((len(train), self.num_trees))\n",
    "        n_samples, n_features = train.shape\n",
    "        sub = int(self.rf * n_features)\n",
    "        \n",
    "        #loop through the length of target y\n",
    "        for i in range(self.num_trees):\n",
    "            #Bootstrap, select len(train) numbers from len(train) numbers, can repete, random\n",
    "            indices = np.random.choice(train.shape[0], train.shape[0], replace = True)\n",
    "            f_indices = np.random.choice(n_features, sub, replace=False)\n",
    "\n",
    "            boot_train = train[indices][:, f_indices] #X\n",
    "            boot_target = target[indices] #y\n",
    "            boot_train_target = np.column_stack((boot_train, boot_target))\n",
    "\n",
    "            #create instance of Tree, fit with bootstraped data\n",
    "            tree = Tree(max_depth=self.max_depth, min_sample_split=self.min_sample_split)\n",
    "            tree.fit(boot_train_target, np.full(boot_train.shape[0], 0), np.full(boot_train.shape[0], 0))\n",
    "            \n",
    "#             score += tree.predict(train)\n",
    "#             score[:, i] = tree.predict(train)\n",
    "            self.trees.append((tree, f_indices))\n",
    "            \n",
    "        \n",
    "#         return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "#         scores = np.zeros((test.shape[0], len(self.trees)))\n",
    "    \n",
    "#         # Make predictions using each tree\n",
    "#         for i, tree in enumerate(self.trees):\n",
    "#             scores[:, i] = tree.predict(test)\n",
    "#         avg_prediction = np.mean(scores, axis=1)  \n",
    "        \n",
    "        if not self.trees:\n",
    "            raise ValueError(\"tree is none.\")\n",
    "\n",
    "        predictions = np.array([tree.predict(test[:, f_indices]) for tree, f_indices in self.trees])\n",
    "        if self.loss == 'mse':\n",
    "            final_predictions = np.mean(predictions, axis=0)\n",
    "        return final_predictions\n",
    "#         return avg_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GBDT:** gradient-based method, lr required\n",
    "\n",
    "**Random Forest:** ensemble method, combines the predictions from multiple decision trees. Each tree trained independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "  \n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        learning_rate = 0.1, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        if loss == 'mse':\n",
    "            self.loss = leastsquare()\n",
    "        elif loss == 'log':\n",
    "            self.loss = logistic()   \n",
    "            \n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, train, target):\n",
    "\n",
    "        pred = np.full(target.shape[0], 0)\n",
    "\n",
    "        g, h = self.learning_rate * self.loss.g(target, pred), self.learning_rate ** 2 *self.loss.h(target, pred)\n",
    "\n",
    "        for _ in range(self.num_trees):\n",
    "\n",
    "            new_tree = Tree(n_threads = self.n_threads)\n",
    "            new_tree.fit(train, g, h)\n",
    "\n",
    "            self.trees.append(new_tree)\n",
    "\n",
    "            pred = self.predict(train)\n",
    "            g, h = self.learning_rate * self.loss.g(target, pred), self.learning_rate ** 2 * self.loss.h(target, pred)\n",
    "#             import pdb\n",
    "#             pdb.set_trace()\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "#         import pdb\n",
    "#         pdb.set_trace()\n",
    "        predictions = np.array([tree.predict(test) for tree in self.trees])\n",
    "        score = np.sum(predictions, axis = 0)\n",
    "        return self.loss.pred(score)\n",
    "#     def fit(self, train, target):\n",
    "\n",
    "#         score = np.full(target.shape[0], 0)\n",
    "#         g = self.learning_rate * self.loss.g(target, score)\n",
    "#         h = (self.learning_rate**2) * self.loss.h(target, score)\n",
    "        \n",
    "#         for _ in range(self.num_trees):\n",
    "            \n",
    "#             #create a tree instance of class Tree, fit Tree\n",
    "#             tree = Tree(n_threads = self.n_threads)\n",
    "#             tree.fit(train, g, h)\n",
    "            \n",
    "#             #update score(prediction), lr*current_score\n",
    "#             score = tree.predict(train)\n",
    "#             g = self.learning_rate * self.loss.g(target, score)\n",
    "#             h = (self.learning_rate**2) * self.loss.h(target, score)\n",
    "        \n",
    "#             self.trees.append(tree)\n",
    "            \n",
    "#         return self\n",
    "\n",
    "#     def predict(self, test):\n",
    "#         #TODO\n",
    "#         scores = np.zeros((test.shape[0],))\n",
    "#         for tree in self.trees:\n",
    "#             scores += self.learning_rate * tree.predict(test)\n",
    "            \n",
    "#         return self.loss.pred(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class Tree(object)**: Class of a single decision tree in GBDT  \n",
    "Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.  \n",
    "        max_depth: The maximum depth of the tree.  \n",
    "        min_sample_split: The minimum number of samples required to further split a node.  \n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.  \n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.  \n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule, rf = 0 means we are training a GBDT. \n",
    "        \n",
    "**fit(self, train, g, h)**:train is the training data matrix, and must be numpy array (an n_train x m matrix). g and h are gradient and hessian respectively.\n",
    "**predict(self,test)**:test is the test data matrix, and must be numpy arrays (an n_test x m matrix). Return predictions (scores) as an array.\n",
    "**construct_tree**：Tree construction，which is recursively used to grow a tree.\n",
    "First we should check if we should stop further splitting.\n",
    "The stopping conditions include:  \n",
    "            1. tree reaches max_depth $d_{max}$  \n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$  \n",
    "            3. gain <= 0\n",
    "            \n",
    "**find_best_decision_rule**： Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, train is the training data assigned to node j, g and h are the corresponding 1st and 2nd derivatives for each data point in traing and h should be vectors of the same length as the number of data points in train, for each feature, we find the best threshold by find_threshold(), a [threshold, best_gain] list is returned for each feature. Then we select the feature with the largest best_gain, and return the best decision rule [feature, treshold] together with its gain.\n",
    "\n",
    "**find_threshold**: Given a particular feature $p_j$, return the best split threshold $\\tau_j$ together with the gain that is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写过了 \n",
    "#TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    #TODO (1, 1, 1, 3, 4) -> (3, 4)\n",
    "#     rmse = np.sqrt(np.mean((pred - y) ** 2))\n",
    "#     return rmse\n",
    "    return np.sqrt(np.mean((pred - y) ** 2))\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "#     #TODO\n",
    "\n",
    "    # <0.5 to 0; > 0.5 to 1\n",
    "    pred = np.round(pred) \n",
    "    return np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.6**  \n",
    "**1. [4 points] What is the computational complexity of optimizing a tree of depth d in terms of m and n?**  \n",
    "<u>Answer</u>: The computational complexity of optimized tree is $O(m \\times n^d)$. At each node, we consider m features, and each feature contains n samples, we iterate over each node into a depth of d.  \n",
    "\n",
    "**2. What operation requires the most expensive computation in GBDT training? Can you suggest a method to improve the efficiency (please do not suggest parallel or distributed computing here since we will discuss it in the next question)? Please give a short description of your method.**  \n",
    "<u>Answer</u>: The most expensive computation in GBDT is arithmetic operation on gradient. One way to improve efficiency is setting a standard for early stopping. If the performance (gain) improvment does not reach this creteria, then the iteration stops early. \n",
    "\n",
    "\n",
    "\n",
    "**3. Which parts of GBDT training can be computed in parallel? Briefly describe your solution, and use it in your implementation**  \n",
    "<u>Answer</u>: Finding the best split point at each node can be done in paralle; Parallelize the training of each tree: train different trees simultaneously subsets of the data by distributing the data across multiple processing units or nodes. Train on each tree, then combine to get the result. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing price dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The RMSE of DGBT and random forest for regression task indicates a better performance than linear regression and ridge regression models. And GBDT performs even better than the random forest regression model:`\n",
    "  \n",
    "Train RMSE for RF is: 3.0152911495325685  \n",
    "Test RMSE for RF is: 4.114077453502243\n",
    "  \n",
    "Training RMSE for GBDT is: 1.7022012576754466  \n",
    "Testing RMSE for GBDT is: 3.3361789632748824  \n",
    "\n",
    "`Recall (pasted from previous HW question answers):`  \n",
    "Train RMSE for linear regression: 4.820626531838223  \n",
    "Train RMSE for ridge regression: 4.82636361174151\n",
    "\n",
    "Train RMSE for linear regression: 5.209217510531067  \n",
    "Train RMSE for ridge regression: 5.191203625647021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,) (354, 13) (354,) (152, 13) (152,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT regression on boston house price dataset\n",
      "Training RMSE for GBDT is: 1.7022012576754466\n",
      "Testing RMSE for GBDT is: 3.3361789632748824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gbdt = GBDT(num_trees = 100, learning_rate = 0.001, n_threads = 1)\n",
    "\n",
    "gbdt.fit(X_train, y_train)\n",
    "\n",
    "pred_train_gbdt = gbdt.predict(X_train)\n",
    "pred_test_gbdt = gbdt.predict(X_test)\n",
    "\n",
    "# Compute RMSE for training and testing\n",
    "rmse_train_gbdt = root_mean_square_error(pred_train_gbdt, y_train)\n",
    "rmse_test_gbdt = root_mean_square_error(pred_test_gbdt, y_test)\n",
    "\n",
    "print(\"GBDT regression on boston house price dataset\")\n",
    "print(\"Training RMSE for GBDT is:\", rmse_train_gbdt)\n",
    "print(\"Testing RMSE for GBDT is:\", rmse_test_gbdt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn GBDT For house price:\n",
      "Train RMSE for GBDT is: 1.2417211555288497\n",
      "Test RMSE for GBDT is: 3.475175659227351\n"
     ]
    }
   ],
   "source": [
    "# double check with sklearn\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBDT\n",
    "\n",
    "gb_regressor = GBDT(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "train_pred_gb = gb_regressor.predict(X_train)\n",
    "test_pred_gb = gb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "train_rmse_gb = root_mean_square_error(train_pred_gb, y_train)\n",
    "test_rmse_gb = root_mean_square_error(test_pred_gb, y_test)\n",
    "\n",
    "print(\"sklearn GBDT For house price:\")\n",
    "\n",
    "print(\"Train RMSE for GBDT is:\", train_rmse_gb)\n",
    "print(\"Test RMSE for GBDT is:\", test_rmse_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE for rf is: 3.0152911495325685\n",
      "Test RMSE for rf is: 4.114077453502243\n"
     ]
    }
   ],
   "source": [
    "rf = RF()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "train_pred_rf = rf.predict(X_train)\n",
    "test_pred_rf = rf.predict(X_test)\n",
    "# print(train_pred_rf)\n",
    "# print(test_pred_rf)\n",
    "\n",
    "train_rmse_rf = root_mean_square_error(train_pred_rf, y_train)\n",
    "# accu_rf = accuracy(pred_rf, y)\n",
    "print(\"Train RMSE for rf is:\", train_rmse_rf)\n",
    "\n",
    "test_rmse_rf = root_mean_square_error(test_pred_rf, y_test)\n",
    "print(\"Test RMSE for rf is:\", test_rmse_rf)\n",
    "\n",
    "# print(\"Accuracy for GBDT is:\", accu_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For house price:\n",
      "Train RMSE for RF is: 2.114212746223618\n",
      "Test RMSE for RF is: 3.824817149581729\n"
     ]
    }
   ],
   "source": [
    "# double check with sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "train_pred_rf = rf.predict(X_train)\n",
    "test_pred_rf = rf.predict(X_test)\n",
    "# print(train_pred_rf)\n",
    "# print(test_pred_rf)\n",
    "train_rmse_rf = root_mean_square_error(train_pred_rf, y_train)\n",
    "# accu_rf = accuracy(pred_rf, y)\n",
    "\n",
    "print(\"For house price:\")\n",
    "print(\"Train RMSE for RF is:\", train_rmse_rf)\n",
    "\n",
    "test_rmse_rf = root_mean_square_error(test_pred_rf, y_test)\n",
    "print(\"Test RMSE for RF is:\", test_rmse_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### credit-g dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT has a better performance than random forest in terms of a higher accuracy. Detailed values of accuracy can be found following each cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,) (700, 20) (700,) (300, 20) (300,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Preprocess the dataset\n",
    "non_numeric = X.select_dtypes(exclude='number').columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in non_numeric:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    X[col] = label_encoders[col].fit_transform(X[col])\n",
    "\n",
    "X = X.values\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# # train-test split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "# print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 15.,  2., ...,  1.,  0.,  1.],\n",
       "       [ 0., 45.,  3., ...,  1.,  0.,  1.],\n",
       "       [ 1., 12.,  3., ...,  1.,  0.,  1.],\n",
       "       ...,\n",
       "       [ 2., 12.,  3., ...,  1.,  0.,  1.],\n",
       "       [ 0., 24.,  1., ...,  1.,  1.,  1.],\n",
       "       [ 3.,  6.,  3., ...,  1.,  0.,  1.]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT For credit-g dataset\n",
      "Train accu for GBDT is: 0.9257142857142857\n",
      "Test accu for GBDT is: 0.7733333333333333\n"
     ]
    }
   ],
   "source": [
    "gb_class = GBDT()\n",
    "gb_class.fit(X_train, y_train)\n",
    "\n",
    "train_pred_gb = gb_class.predict(X_train) > 0.5\n",
    "test_pred_gb = gb_class.predict(X_test) > 0.5\n",
    "\n",
    "# Calculate RMSE\n",
    "accu_train = accuracy(train_pred_gb, y_train)\n",
    "accu_test = accuracy(test_pred_gb, y_test)\n",
    "\n",
    "print(\"GBDT For credit-g dataset\")\n",
    "print(\"Train accu for GBDT is:\", accu_train)\n",
    "print(\"Test accu for GBDT is:\", accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT For credit-g dataset\n",
      "Train accu for rf is: 0.7357142857142858\n",
      "Test accu for rf is: 0.7166666666666667\n"
     ]
    }
   ],
   "source": [
    "rf_class = RF(n_estimators=100, max_depth=3)\n",
    "rf_class.fit(X_train, y_train)\n",
    "\n",
    "train_pred_rf = rf_class.predict(X_train) > 0.5\n",
    "test_pred_rf =rf_class.predict(X_test) > 0.5\n",
    "\n",
    "# Calculate RMSE\n",
    "accu_train = accuracy(train_pred_rf, y_train)\n",
    "accu_test = accuracy(test_pred_rf, y_test)\n",
    "\n",
    "print(\"GBDT For credit-g dataset\")\n",
    "print(\"Train accu for rf is:\", accu_train)\n",
    "print(\"Test accu for rf is:\", accu_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT has a better performance than random forest in terms of a higher accuracy. Detailed values of accuracy can be found following each cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,) (398, 30) (398,) (171, 30) (171,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT classification on breast cancer dataset\n",
      "Train accu for GBDT is: 1.0\n",
      "Test accu for GBDT is: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "gb_class = GBDT()\n",
    "gb_class.fit(X_train, y_train)\n",
    "\n",
    "threshold = 0.5\n",
    "gbdt_pred_train = gb_class.predict(X_train) > threshold\n",
    "gbdt_acc_train = accuracy(gbdt_pred_train, y_train)\n",
    "\n",
    "gbdt_pred_test = gb_class.predict(X_test) > threshold\n",
    "gbdt_acc_test = accuracy(gbdt_pred_test, y_test)\n",
    "\n",
    "print(\"GBDT classification on breast cancer dataset\")\n",
    "print(\"Train accu for GBDT is:\", gbdt_acc_train)\n",
    "print(\"Test accu for GBDT is:\", gbdt_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn GBDT For breast cancer dataset:\n",
      "Train accu for GBDT is: 1.0\n",
      "Test accu for GBDT is: 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "gb_class = GBDT(n_estimators=100, learning_rate=0.9, max_depth=3, random_state=42)\n",
    "gb_class.fit(X_train, y_train)\n",
    "\n",
    "train_pred_gb = gb_class.predict(X_train) > 0.5\n",
    "test_pred_gb = gb_class.predict(X_test) > 0.5\n",
    "\n",
    "# Calculate RMSE\n",
    "accu_train = accuracy(train_pred_gb, y_train)\n",
    "accu_test = accuracy(test_pred_gb, y_test)\n",
    "\n",
    "print(\"sklearn GBDT For breast cancer dataset:\")\n",
    "print(\"Train accu for GBDT is:\", accu_train)\n",
    "print(\"Test accu for GBDT is:\", accu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF classification on breast cancer dataset:\n",
      "Train accuracy for rf is: 1.0\n",
      "Test accuracy for rf is: 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "rf = RF()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "train_pred_rf = rf.predict(X_train) > 0.5\n",
    "test_pred_rf = rf.predict(X_test)> 0.5\n",
    "# print(train_pred_rf)\n",
    "# print(test_pred_rf)\n",
    "\n",
    "train_accu_rf = accuracy(train_pred_rf, y_train)\n",
    "test_accu_rf = accuracy(test_pred_rf, y_test)\n",
    "\n",
    "# accu_rf = accuracy(pred_rf, y)\n",
    "print(\"RF classification on breast cancer dataset:\")\n",
    "print(\"Train accuracy for rf is:\", train_accu_rf)\n",
    "\n",
    "print(\"Test accuracy for rf is:\", test_accu_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn RF for breast cancer dataset:\n",
      "Train accu for rf is: 0.99\n",
      "Test accu for rf is: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "# y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "rf = RF(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "train_pred_rf = rf.predict(X_train) > 0.5\n",
    "test_pred_rf = rf.predict(X_test) > 0.5\n",
    "# print(train_pred_rf)\n",
    "# print(test_pred_rf)\n",
    "train_accu_rf = round(accuracy(train_pred_rf, y_train),2)\n",
    "test_accu_rf = round(accuracy(test_pred_rf, y_test),2)\n",
    "\n",
    "print(\"sklearn RF for breast cancer dataset:\")\n",
    "\n",
    "print(\"Train accu for rf is:\", train_accu_rf)\n",
    "print(\"Test accu for rf is:\", test_accu_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT captures complex relationships within data compared to Random Forest. Since GBDT builds trees and continiously correct errors made by previous ones, it can capture more complexity. This characteristic can potentially be more accurate when having a complex dataset.  \n",
    "But GBDT minimizes loss function using gradient descent and predict iteratively, which can lead to better performance, especially with complex data.  \n",
    "Random Forest builds multiple trees independently, leading to poorer response to complex data relationship.  \n",
    "Hyperparameter tuning can a;so influence the result. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
